{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a Model on a Task Dataset\n",
    "\n",
    "In this example, we're going to walk through how to run a model on a specific task dataset. We'll use the `transformer` base model, and run on the `Secondary Structure` dataset. To do this, we'll need a few things:\n",
    "\n",
    "- Dataset\n",
    "- DataLoader + CollateFn\n",
    "- TAPEConfig\n",
    "- TaskModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Let's start by creating the `SecondaryStructureDataset`. This is located in the `tape_pytorch.datasets` module.\n",
    "\n",
    "There are three arguments that every TAPE task dataset has. The first, `data_path` is the path to the TAPE data directory. By default, we assume this is a folder called `data` in the top level of the `tape-pytorch` directory, but you can put it wherever you like. Note that this is *not* the path to the task-specific data, which is located inside the TAPE data directory and must be named appropriately.\n",
    "\n",
    "The second argument, `mode`, refers to the particular dataset split to train on. For some tasks this is simply, `train`, `val`, and `test`. For other tasks this can also refer to specific test splits (e.g. for the Secondary Structure task, the test splits are `cb513`, `ts115`, and `casp12`).\n",
    "\n",
    "The third argument, `tokenizer`, refers to how input sequences should be tokenized. There are two options here, `dummy`, which splits sequences into individual amino acids, and `bpe`, which uses a byte-pair encoding of the input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tape_pytorch.datasets import SecondaryStructureDataset\n",
    "\n",
    "dataset = SecondaryStructureDataset(data_path='../data', mode='train', tokenizer='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader + CollateFn\n",
    "\n",
    "The DataLoader used in TAPE is the standard [torch DataLoader](https://pytorch.org/docs/stable/data.html). However, we also need a custom [collate_fn](https://pytorch.org/docs/stable/data.html#working-with-collate-fn) because we have variable length sequences. Task-specific collate functions can also be loaded from the `tape_pytorch.datasets` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tape_pytorch.datasets import SecondaryStructureBatch\n",
    "\n",
    "batch_size = 4\n",
    "num_workers = 1\n",
    "collate_fn = SecondaryStructureBatch()\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAPEConfig\n",
    "\n",
    "A TAPE Config object is used to configure the base model for each TAPE task. It can be imported from `tape_pytorch.models`. It provides a number of configuration options. The best way to set these are with config files. See `tape-pytorch/config/transformer_config.json`, which is the file we'll be using to load a config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tape_pytorch.models import TAPEConfig\n",
    "\n",
    "config = TAPEConfig.from_json_file('../config/transformer_config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TaskModel\n",
    "\n",
    "The final part is to create the task model! These are located in `tape_pytorch.models.task_models`. The model we need for Secondary Structure prediction is called the `SequenceToSequenceClassificationModel`. Let's make it now. We'll also need to add a config option to `SequenceToSequenceClassificationModel`, which tells it the number of classes we want to predict. Since we're doing 3-class secondary structure prediction, we set `config.num_classes = 3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tape_pytorch.models.task_models import SequenceToSequenceClassificationModel\n",
    "\n",
    "config.num_classes = 3\n",
    "task_model = SequenceToSequenceClassificationModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model\n",
    "\n",
    "And that's it - now we can run the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 265]) torch.Size([4, 263])\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "loss = 0\n",
    "batch = next(iter(dataloader))\n",
    "outputs = task_model(**batch)\n",
    "print(outputs.keys())\n",
    "print('Loss:', outputs[task_model.LOSS_KEY])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Easier Way\n",
    "\n",
    "There's a lot of things to remember here, and it would be nice if it was possible to do this in a simpler way. Fortunately, there is! TAPE has a `registry`, which stores all object classes of interest, and lets you access them by providing just the task name. So the code above turns into this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tape_pytorch.registry import registry\n",
    "from torch.utils.data import DataLoader\n",
    "from tape_pytorch.models import TAPEConfig\n",
    "\n",
    "task_name = 'secondary_structure'\n",
    "\n",
    "dataset = registry.get_dataset_class(task_name)(data_path='../data', mode='train', tokenizer='dummy')\n",
    "collate_fn = registry.get_collate_fn_class(task_name)()\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n",
    "config = TAPEConfig.from_json_file('../config/transformer_config.json')\n",
    "config.num_classes = 3\n",
    "task_model = registry.get_task_model_class(task_name)(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
